{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upworthy Archive Sentiment Code (1/2)\n",
    "This notebook covers the code used to process the Upworthy Archive exploratory data and find simple OLS regressions of various classifiers (in Python). Python does not have a sufficient package for analyzing time-invariant non-demeaned data (PanelOLS doesn't work with this), so we turn to R to find Fixed Effects in the other notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "string.punctuation\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load upworthy archive and instantiate CTR and and headline variables\n",
    "upworthy_data_dir = \"upworthy-archive-exploratory-packages-03.12.2020.csv\"\n",
    "upworthy = pd.read_csv(upworthy_data_dir, index_col = 1)\n",
    "CTR = np.array(upworthy['clicks'])/np.array(upworthy['impressions'])\n",
    "upworthy['CTR'] = CTR\n",
    "headline = upworthy.headline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk downloader if you don't have it already (run once only)\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>clickability_test_id</th>\n",
       "      <th>excerpt</th>\n",
       "      <th>headline</th>\n",
       "      <th>lede</th>\n",
       "      <th>slug</th>\n",
       "      <th>eyecatcher_id</th>\n",
       "      <th>impressions</th>\n",
       "      <th>clicks</th>\n",
       "      <th>significance</th>\n",
       "      <th>first_place</th>\n",
       "      <th>winner</th>\n",
       "      <th>share_text</th>\n",
       "      <th>square</th>\n",
       "      <th>test_week</th>\n",
       "      <th>CTR</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>created_at</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-11-20 06:43:16.005</th>\n",
       "      <td>0</td>\n",
       "      <td>2016-04-02 16:33:38.062</td>\n",
       "      <td>546d88fb84ad38b2ce000024</td>\n",
       "      <td>Things that matter. Pass 'em on.</td>\n",
       "      <td>They're Being Called 'Walmart's Worst Nightmar...</td>\n",
       "      <td>&lt;p&gt;When I saw *why* people are calling them \"W...</td>\n",
       "      <td>theyre-being-called-walmarts-worst-nightmare-a...</td>\n",
       "      <td>546d6fa19ad54eec8d00002d</td>\n",
       "      <td>3052</td>\n",
       "      <td>150</td>\n",
       "      <td>100.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Anyone who's ever felt guilty about shopping a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>201446</td>\n",
       "      <td>0.049148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-11-20 06:43:44.646</th>\n",
       "      <td>1</td>\n",
       "      <td>2016-04-02 16:25:54.021</td>\n",
       "      <td>546d88fb84ad38b2ce000024</td>\n",
       "      <td>Things that matter. Pass 'em on.</td>\n",
       "      <td>They're Being Called 'Walmart's Worst Nightmar...</td>\n",
       "      <td>&lt;p&gt;When I saw *why* people are calling them \"W...</td>\n",
       "      <td>theyre-being-called-walmarts-worst-nightmare-a...</td>\n",
       "      <td>546d6fa19ad54eec8d00002d</td>\n",
       "      <td>3033</td>\n",
       "      <td>122</td>\n",
       "      <td>14.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Walmart is getting schooled by another retaile...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>201446</td>\n",
       "      <td>0.040224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-11-20 06:44:59.804</th>\n",
       "      <td>2</td>\n",
       "      <td>2016-04-02 16:25:54.024</td>\n",
       "      <td>546d88fb84ad38b2ce000024</td>\n",
       "      <td>Things that matter. Pass 'em on.</td>\n",
       "      <td>They're Being Called 'Walmart's Worst Nightmar...</td>\n",
       "      <td>&lt;p&gt;When I saw *why* people are calling them \"W...</td>\n",
       "      <td>theyre-being-called-walmarts-worst-nightmare-a...</td>\n",
       "      <td>546d6fa19ad54eec8d00002d</td>\n",
       "      <td>3092</td>\n",
       "      <td>110</td>\n",
       "      <td>1.8</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Walmart may not be crapping their pants over t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>201446</td>\n",
       "      <td>0.035576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-11-20 06:54:36.335</th>\n",
       "      <td>3</td>\n",
       "      <td>2016-04-02 16:25:54.027</td>\n",
       "      <td>546d902c26714c6c44000039</td>\n",
       "      <td>Things that matter. Pass 'em on.</td>\n",
       "      <td>This Is What Sexism Against Men Sounds Like</td>\n",
       "      <td>&lt;p&gt;DISCLOSURE: I'm a dude. I have cried on mul...</td>\n",
       "      <td>this-is-what-sexism-against-men-sounds-like-am...</td>\n",
       "      <td>546bc55335992b86c8000043</td>\n",
       "      <td>3526</td>\n",
       "      <td>90</td>\n",
       "      <td>4.1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>If you ever wondered, \"but what about the men?...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>201446</td>\n",
       "      <td>0.025525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-11-20 06:54:57.878</th>\n",
       "      <td>4</td>\n",
       "      <td>2016-04-02 16:31:45.671</td>\n",
       "      <td>546d902c26714c6c44000039</td>\n",
       "      <td>Things that matter. Pass 'em on.</td>\n",
       "      <td>This Is What Sexism Against Men Sounds Like</td>\n",
       "      <td>&lt;p&gt;DISCLOSURE: I'm a dude. I have cried on mul...</td>\n",
       "      <td>this-is-what-sexism-against-men-sounds-like-am...</td>\n",
       "      <td>546d900426714cd2dd00002e</td>\n",
       "      <td>3506</td>\n",
       "      <td>120</td>\n",
       "      <td>100.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>If you ever wondered, \"but what about the men?...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>201446</td>\n",
       "      <td>0.034227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-11-20 01:21:49.197</th>\n",
       "      <td>150749</td>\n",
       "      <td>2016-04-02 16:25:53.916</td>\n",
       "      <td>546d373426714cde76000018</td>\n",
       "      <td>Things that matter. Pass 'em on.</td>\n",
       "      <td>5 Reasons You May Need To Plan A Vacation - Ri...</td>\n",
       "      <td>&lt;p&gt;Travel isn't just a luxury or indulgence an...</td>\n",
       "      <td>5-reasons-you-may-need-to-plan-a-vacation-righ...</td>\n",
       "      <td>546d398a9ad54eec8d00000f</td>\n",
       "      <td>3724</td>\n",
       "      <td>18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>201446</td>\n",
       "      <td>0.004834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-11-20 01:23:31.203</th>\n",
       "      <td>150755</td>\n",
       "      <td>2016-04-02 16:31:40.651</td>\n",
       "      <td>546d373426714cde76000018</td>\n",
       "      <td>Things that matter. Pass 'em on.</td>\n",
       "      <td>The Next Time You Encounter A Small Minded Big...</td>\n",
       "      <td>&lt;p&gt;Travel isn't just a luxury or indulgence an...</td>\n",
       "      <td>the-next-time-you-encounter-a-small-minded-big...</td>\n",
       "      <td>546d398a9ad54eec8d00000f</td>\n",
       "      <td>3728</td>\n",
       "      <td>23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>201446</td>\n",
       "      <td>0.006170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-11-20 01:24:23.415</th>\n",
       "      <td>150756</td>\n",
       "      <td>2016-04-02 16:31:50.079</td>\n",
       "      <td>546d373426714cde76000018</td>\n",
       "      <td>Things that matter. Pass 'em on.</td>\n",
       "      <td>I've Never Wanted To Buy A Plane Ticket More T...</td>\n",
       "      <td>&lt;p&gt;Travel isn't just a luxury or indulgence an...</td>\n",
       "      <td>ive-never-wanted-to-buy-a-plane-ticket-more-th...</td>\n",
       "      <td>546d398a9ad54eec8d00000f</td>\n",
       "      <td>3581</td>\n",
       "      <td>21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>201446</td>\n",
       "      <td>0.005864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-14 17:11:40.585</th>\n",
       "      <td>150813</td>\n",
       "      <td>2016-04-02 16:24:27.29</td>\n",
       "      <td>54b6a21662646300182c0000</td>\n",
       "      <td>It all makes sense now.</td>\n",
       "      <td>3 Ladies Having Too Much Fun At The Epicenter ...</td>\n",
       "      <td>&lt;p&gt;The Frackettes want to remind you of one im...</td>\n",
       "      <td>3-ladies-having-too-much-fun-at-the-epicenter-...</td>\n",
       "      <td>54b6a2df3931650012620000</td>\n",
       "      <td>3425</td>\n",
       "      <td>37</td>\n",
       "      <td>100.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>201502</td>\n",
       "      <td>0.010803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-11-20 06:23:55.115</th>\n",
       "      <td>150816</td>\n",
       "      <td>2016-04-02 16:25:54.018</td>\n",
       "      <td>546d88fb84ad38b2ce000024</td>\n",
       "      <td>Things that matter. Pass 'em on.</td>\n",
       "      <td>They're Being Called 'Walmart's Worst Nightmar...</td>\n",
       "      <td>&lt;p&gt;When I saw *why* people are calling them \"W...</td>\n",
       "      <td>theyre-being-called-walmarts-worst-nightmare-a...</td>\n",
       "      <td>546d6fa19ad54eec8d00002d</td>\n",
       "      <td>3018</td>\n",
       "      <td>132</td>\n",
       "      <td>16.2</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>201446</td>\n",
       "      <td>0.043738</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22666 rows Ã— 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Unnamed: 0               updated_at  \\\n",
       "created_at                                                     \n",
       "2014-11-20 06:43:16.005           0  2016-04-02 16:33:38.062   \n",
       "2014-11-20 06:43:44.646           1  2016-04-02 16:25:54.021   \n",
       "2014-11-20 06:44:59.804           2  2016-04-02 16:25:54.024   \n",
       "2014-11-20 06:54:36.335           3  2016-04-02 16:25:54.027   \n",
       "2014-11-20 06:54:57.878           4  2016-04-02 16:31:45.671   \n",
       "...                             ...                      ...   \n",
       "2014-11-20 01:21:49.197      150749  2016-04-02 16:25:53.916   \n",
       "2014-11-20 01:23:31.203      150755  2016-04-02 16:31:40.651   \n",
       "2014-11-20 01:24:23.415      150756  2016-04-02 16:31:50.079   \n",
       "2015-01-14 17:11:40.585      150813   2016-04-02 16:24:27.29   \n",
       "2014-11-20 06:23:55.115      150816  2016-04-02 16:25:54.018   \n",
       "\n",
       "                             clickability_test_id  \\\n",
       "created_at                                          \n",
       "2014-11-20 06:43:16.005  546d88fb84ad38b2ce000024   \n",
       "2014-11-20 06:43:44.646  546d88fb84ad38b2ce000024   \n",
       "2014-11-20 06:44:59.804  546d88fb84ad38b2ce000024   \n",
       "2014-11-20 06:54:36.335  546d902c26714c6c44000039   \n",
       "2014-11-20 06:54:57.878  546d902c26714c6c44000039   \n",
       "...                                           ...   \n",
       "2014-11-20 01:21:49.197  546d373426714cde76000018   \n",
       "2014-11-20 01:23:31.203  546d373426714cde76000018   \n",
       "2014-11-20 01:24:23.415  546d373426714cde76000018   \n",
       "2015-01-14 17:11:40.585  54b6a21662646300182c0000   \n",
       "2014-11-20 06:23:55.115  546d88fb84ad38b2ce000024   \n",
       "\n",
       "                                                  excerpt  \\\n",
       "created_at                                                  \n",
       "2014-11-20 06:43:16.005  Things that matter. Pass 'em on.   \n",
       "2014-11-20 06:43:44.646  Things that matter. Pass 'em on.   \n",
       "2014-11-20 06:44:59.804  Things that matter. Pass 'em on.   \n",
       "2014-11-20 06:54:36.335  Things that matter. Pass 'em on.   \n",
       "2014-11-20 06:54:57.878  Things that matter. Pass 'em on.   \n",
       "...                                                   ...   \n",
       "2014-11-20 01:21:49.197  Things that matter. Pass 'em on.   \n",
       "2014-11-20 01:23:31.203  Things that matter. Pass 'em on.   \n",
       "2014-11-20 01:24:23.415  Things that matter. Pass 'em on.   \n",
       "2015-01-14 17:11:40.585           It all makes sense now.   \n",
       "2014-11-20 06:23:55.115  Things that matter. Pass 'em on.   \n",
       "\n",
       "                                                                  headline  \\\n",
       "created_at                                                                   \n",
       "2014-11-20 06:43:16.005  They're Being Called 'Walmart's Worst Nightmar...   \n",
       "2014-11-20 06:43:44.646  They're Being Called 'Walmart's Worst Nightmar...   \n",
       "2014-11-20 06:44:59.804  They're Being Called 'Walmart's Worst Nightmar...   \n",
       "2014-11-20 06:54:36.335        This Is What Sexism Against Men Sounds Like   \n",
       "2014-11-20 06:54:57.878        This Is What Sexism Against Men Sounds Like   \n",
       "...                                                                    ...   \n",
       "2014-11-20 01:21:49.197  5 Reasons You May Need To Plan A Vacation - Ri...   \n",
       "2014-11-20 01:23:31.203  The Next Time You Encounter A Small Minded Big...   \n",
       "2014-11-20 01:24:23.415  I've Never Wanted To Buy A Plane Ticket More T...   \n",
       "2015-01-14 17:11:40.585  3 Ladies Having Too Much Fun At The Epicenter ...   \n",
       "2014-11-20 06:23:55.115  They're Being Called 'Walmart's Worst Nightmar...   \n",
       "\n",
       "                                                                      lede  \\\n",
       "created_at                                                                   \n",
       "2014-11-20 06:43:16.005  <p>When I saw *why* people are calling them \"W...   \n",
       "2014-11-20 06:43:44.646  <p>When I saw *why* people are calling them \"W...   \n",
       "2014-11-20 06:44:59.804  <p>When I saw *why* people are calling them \"W...   \n",
       "2014-11-20 06:54:36.335  <p>DISCLOSURE: I'm a dude. I have cried on mul...   \n",
       "2014-11-20 06:54:57.878  <p>DISCLOSURE: I'm a dude. I have cried on mul...   \n",
       "...                                                                    ...   \n",
       "2014-11-20 01:21:49.197  <p>Travel isn't just a luxury or indulgence an...   \n",
       "2014-11-20 01:23:31.203  <p>Travel isn't just a luxury or indulgence an...   \n",
       "2014-11-20 01:24:23.415  <p>Travel isn't just a luxury or indulgence an...   \n",
       "2015-01-14 17:11:40.585  <p>The Frackettes want to remind you of one im...   \n",
       "2014-11-20 06:23:55.115  <p>When I saw *why* people are calling them \"W...   \n",
       "\n",
       "                                                                      slug  \\\n",
       "created_at                                                                   \n",
       "2014-11-20 06:43:16.005  theyre-being-called-walmarts-worst-nightmare-a...   \n",
       "2014-11-20 06:43:44.646  theyre-being-called-walmarts-worst-nightmare-a...   \n",
       "2014-11-20 06:44:59.804  theyre-being-called-walmarts-worst-nightmare-a...   \n",
       "2014-11-20 06:54:36.335  this-is-what-sexism-against-men-sounds-like-am...   \n",
       "2014-11-20 06:54:57.878  this-is-what-sexism-against-men-sounds-like-am...   \n",
       "...                                                                    ...   \n",
       "2014-11-20 01:21:49.197  5-reasons-you-may-need-to-plan-a-vacation-righ...   \n",
       "2014-11-20 01:23:31.203  the-next-time-you-encounter-a-small-minded-big...   \n",
       "2014-11-20 01:24:23.415  ive-never-wanted-to-buy-a-plane-ticket-more-th...   \n",
       "2015-01-14 17:11:40.585  3-ladies-having-too-much-fun-at-the-epicenter-...   \n",
       "2014-11-20 06:23:55.115  theyre-being-called-walmarts-worst-nightmare-a...   \n",
       "\n",
       "                                    eyecatcher_id  impressions  clicks  \\\n",
       "created_at                                                               \n",
       "2014-11-20 06:43:16.005  546d6fa19ad54eec8d00002d         3052     150   \n",
       "2014-11-20 06:43:44.646  546d6fa19ad54eec8d00002d         3033     122   \n",
       "2014-11-20 06:44:59.804  546d6fa19ad54eec8d00002d         3092     110   \n",
       "2014-11-20 06:54:36.335  546bc55335992b86c8000043         3526      90   \n",
       "2014-11-20 06:54:57.878  546d900426714cd2dd00002e         3506     120   \n",
       "...                                           ...          ...     ...   \n",
       "2014-11-20 01:21:49.197  546d398a9ad54eec8d00000f         3724      18   \n",
       "2014-11-20 01:23:31.203  546d398a9ad54eec8d00000f         3728      23   \n",
       "2014-11-20 01:24:23.415  546d398a9ad54eec8d00000f         3581      21   \n",
       "2015-01-14 17:11:40.585  54b6a2df3931650012620000         3425      37   \n",
       "2014-11-20 06:23:55.115  546d6fa19ad54eec8d00002d         3018     132   \n",
       "\n",
       "                         significance  first_place  winner  \\\n",
       "created_at                                                   \n",
       "2014-11-20 06:43:16.005         100.0         True    True   \n",
       "2014-11-20 06:43:44.646          14.0        False   False   \n",
       "2014-11-20 06:44:59.804           1.8        False   False   \n",
       "2014-11-20 06:54:36.335           4.1        False   False   \n",
       "2014-11-20 06:54:57.878         100.0         True   False   \n",
       "...                               ...          ...     ...   \n",
       "2014-11-20 01:21:49.197           0.0         True   False   \n",
       "2014-11-20 01:23:31.203           0.0        False   False   \n",
       "2014-11-20 01:24:23.415           0.0        False   False   \n",
       "2015-01-14 17:11:40.585         100.0         True   False   \n",
       "2014-11-20 06:23:55.115          16.2        False   False   \n",
       "\n",
       "                                                                share_text  \\\n",
       "created_at                                                                   \n",
       "2014-11-20 06:43:16.005  Anyone who's ever felt guilty about shopping a...   \n",
       "2014-11-20 06:43:44.646  Walmart is getting schooled by another retaile...   \n",
       "2014-11-20 06:44:59.804  Walmart may not be crapping their pants over t...   \n",
       "2014-11-20 06:54:36.335  If you ever wondered, \"but what about the men?...   \n",
       "2014-11-20 06:54:57.878  If you ever wondered, \"but what about the men?...   \n",
       "...                                                                    ...   \n",
       "2014-11-20 01:21:49.197                                                NaN   \n",
       "2014-11-20 01:23:31.203                                                NaN   \n",
       "2014-11-20 01:24:23.415                                                NaN   \n",
       "2015-01-14 17:11:40.585                                                NaN   \n",
       "2014-11-20 06:23:55.115                                                NaN   \n",
       "\n",
       "                        square  test_week       CTR  \n",
       "created_at                                           \n",
       "2014-11-20 06:43:16.005    NaN     201446  0.049148  \n",
       "2014-11-20 06:43:44.646    NaN     201446  0.040224  \n",
       "2014-11-20 06:44:59.804    NaN     201446  0.035576  \n",
       "2014-11-20 06:54:36.335    NaN     201446  0.025525  \n",
       "2014-11-20 06:54:57.878    NaN     201446  0.034227  \n",
       "...                        ...        ...       ...  \n",
       "2014-11-20 01:21:49.197    NaN     201446  0.004834  \n",
       "2014-11-20 01:23:31.203    NaN     201446  0.006170  \n",
       "2014-11-20 01:24:23.415    NaN     201446  0.005864  \n",
       "2015-01-14 17:11:40.585    NaN     201502  0.010803  \n",
       "2014-11-20 06:23:55.115    NaN     201446  0.043738  \n",
       "\n",
       "[22666 rows x 17 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upworthy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posemo/Negemo Dictionary Model\n",
    "This was a first \"foray,\" if you will, of trying to tackle sentiment. We will not be doing further analysis of this model at the fixed-effects stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load posemo dictionary\n",
    "\n",
    "posemo_data_dir = \"posemo_dict.csv\"\n",
    "posemo = pd.read_csv(posemo_data_dir, index_col = 0).word\n",
    "#posemo.head()\n",
    "posemo_punct = [word.strip(string.punctuation) for word in posemo][2:]\n",
    "posemo_clean = [word.replace(r\")\",\"\").replace(r\"(\",\"\") for word in posemo_punct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load negemo dictionary\n",
    "negemo_data_dir = \"negemo_dict.csv\"\n",
    "negemo = pd.read_csv(negemo_data_dir, index_col = 0).word\n",
    "#negemo.head()\n",
    "negemo_punct = [word.strip(string.punctuation) for word in negemo][2:]\n",
    "negemo_clean = [word.replace(r\")\",\"\").replace(r\"(\",\"\") for word in negemo_punct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load stopwords dictionary\n",
    "stop = stopwords.words('english')\n",
    "stop_list = set(stop)\n",
    "exception_stop = set(['s',\"t\",\"no\",\"d\",\"ll\",\"m\",\"o\",\"re\",\"ve\",\"y\",\"won\",\"ma\",\"not\"])\n",
    "add_stop = set([\"what's\"])\n",
    "stop = list((stop_list.union(add_stop))-exception_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split headlines\n",
    "headline_wordlist = [0]*len(headline)\n",
    "for i in range(len(headline)):\n",
    "    headline_wordlist[i] = headline[i].split()\n",
    "    \n",
    "#clean headlines of unnecessary punctuation\n",
    "for headline_num in range(len(headline_wordlist)):\n",
    "    for word_num in range(len(headline_wordlist[headline_num])):\n",
    "        headline_wordlist[headline_num][word_num] = headline_wordlist[headline_num][word_num].strip(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove extraneous grammar words\n",
    "for i in range(len(headline_wordlist)):\n",
    "    for j in range(len(headline_wordlist[i])):\n",
    "        for k in range(len(stop)):\n",
    "            if re.fullmatch(stop[k],headline_wordlist[i][j].lower()) != None:\n",
    "                headline_wordlist[i][j] = \"\"\n",
    "    headline_wordlist[i] = list(filter(None,headline_wordlist[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing postive matching algorithm on headlines\n",
    "posemo_counts = []\n",
    "for headline_blurb in range(len(headline_wordlist)):\n",
    "    for headline_word in range(len(headline_wordlist[headline_blurb])):\n",
    "        for posemo_word in posemo_clean:\n",
    "            if re.search(posemo_word, headline_wordlist[headline_blurb][headline_word][0:len(posemo_word)]) != None:\n",
    "#                print(str(headline_blurb) + \", \" + str(posemo_word) + \", \" + str(headline_wordlist[headline_blurb][headline_word]))\n",
    "                posemo_counts.append(headline_blurb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add posemo counter to dataframe\n",
    "headline_posemo_counter = [0]*len(headline)\n",
    "\n",
    "for headline_num in posemo_counts:\n",
    "    headline_posemo_counter[headline_num] += 1\n",
    "    \n",
    "upworthy[\"posemo_counts\"] = headline_posemo_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing negative matching algorithm on headlines\n",
    "negemo_counts = []\n",
    "for headline_blurb in range(len(headline_wordlist)):\n",
    "    for headline_word in range(len(headline_wordlist[headline_blurb])):\n",
    "        for negemo_word in negemo_clean:\n",
    "            if re.search(negemo_word, headline_wordlist[headline_blurb][headline_word][0:len(negemo_word)]) != None:\n",
    "#                print(str(headline_blurb) + \", \" + str(negemo_word) + \", \" + str(headline_wordlist[headline_blurb][headline_word]))\n",
    "                negemo_counts.append(headline_blurb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add negemo counter to dataframe\n",
    "headline_negemo_counter = [0]*len(headline)\n",
    "\n",
    "for headline_num in negemo_counts:\n",
    "    headline_negemo_counter[headline_num] += 1\n",
    "    \n",
    "upworthy[\"negemo_counts\"] = headline_negemo_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classify headline into positive, negative, or neutral and add to headline\n",
    "polarity = np.array(headline_posemo_counter) - np.array(headline_negemo_counter)\n",
    "headline_polarity = [0]*len(headline)\n",
    "\n",
    "for i in range(len(polarity)):\n",
    "    if polarity[i] > 0:\n",
    "        headline_polarity[i] = \"Positive\"\n",
    "    elif polarity[i] < 0:\n",
    "        headline_polarity[i] = \"Negative\"\n",
    "    else:\n",
    "        headline_polarity[i] = \"Baseline_Neutral\"\n",
    "\n",
    "upworthy[\"polarity\"] = headline_polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                    CTR   R-squared:                       0.008\n",
      "Model:                            OLS   Adj. R-squared:                  0.008\n",
      "Method:                 Least Squares   F-statistic:                     96.49\n",
      "Date:                Wed, 20 Jan 2021   Prob (F-statistic):           1.88e-42\n",
      "Time:                        15:14:54   Log-Likelihood:                 67644.\n",
      "No. Observations:               22666   AIC:                        -1.353e+05\n",
      "Df Residuals:                   22663   BIC:                        -1.353e+05\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "===========================================================================================\n",
      "                              coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------------\n",
      "Intercept                   0.0160   8.28e-05    192.958      0.000       0.016       0.016\n",
      "C(polarity)[T.Negative]    -0.0057      0.001     -7.904      0.000      -0.007      -0.004\n",
      "C(polarity)[T.Positive]    -0.0063      0.001    -11.560      0.000      -0.007      -0.005\n",
      "==============================================================================\n",
      "Omnibus:                    10679.564   Durbin-Watson:                   0.596\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            75418.633\n",
      "Skew:                           2.157   Prob(JB):                         0.00\n",
      "Kurtosis:                      10.826   Cond. No.                         8.90\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "# Run OLS regression with polarity as indicator variable. Source: https://www.statsmodels.org/devel/generated/statsmodels.regression.linear_model.OLS.html\n",
    "\n",
    "df_dict = upworthy[['CTR','polarity']]\n",
    "\n",
    "polarity_dict = smf.ols(formula = 'CTR ~ C(polarity)', data = df_dict).fit()\n",
    "print(polarity_dict.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VADER Models\n",
    "This takes a sophisticated sentiment analyzer and finds if there is significance between sentiment and CTR. Two models were made: one of the raw polarity scores, and one with the analyzer's judgement score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load fresh DataFrame\n",
    "upworthy = pd.read_csv(upworthy_data_dir, index_col = 1)\n",
    "CTR = np.array(upworthy['clicks'])/np.array(upworthy['impressions'])\n",
    "upworthy['CTR'] = CTR\n",
    "headline = upworthy.headline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load vader sentiment analyzer\n",
    "vader = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get pos, neg, neu, and compound scores from vader\n",
    "def vader_neutral(text):\n",
    "    score = vader.polarity_scores(text)\n",
    "    return score['neu']\n",
    "\n",
    "def vader_pos(text):\n",
    "    score = vader.polarity_scores(text)\n",
    "    return score['pos']\n",
    "\n",
    "def vader_neg(text):\n",
    "    score = vader.polarity_scores(text)\n",
    "    return score['neg']\n",
    "\n",
    "def vader_compound(text):\n",
    "    score = vader.polarity_scores(text)\n",
    "    return score['compound']\n",
    "\n",
    "polarity_neu = [0]*len(upworthy)  \n",
    "polarity_pos = [0]*len(upworthy)  \n",
    "polarity_neg = [0]*len(upworthy)  \n",
    "polarity_compound = [0]*len(upworthy)\n",
    "\n",
    "for i in range(len(headline)):\n",
    "    polarity_neu[i] = vader_neutral(headline[i])\n",
    "    polarity_pos[i] = vader_pos(headline[i])\n",
    "    polarity_neg[i] = vader_neg(headline[i])\n",
    "    polarity_compound[i] = vader_compound(headline[i])\n",
    "\n",
    "upworthy['Neutral'] = polarity_neu\n",
    "upworthy['Positive'] = polarity_pos\n",
    "upworthy['Negative'] = polarity_neg\n",
    "upworthy['compound'] = polarity_compound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classifier with defined thresholds, using pos neg neu scores\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "def vader_polarity_pnn(text):\n",
    "    score = vader.polarity_scores(text)\n",
    "    if score['neu'] >=0.85:\n",
    "        return 'Baseline_Neutral'\n",
    "    elif score['pos'] >= 0.15 and score['pos'] > score['neg']:\n",
    "        return 'Positive'\n",
    "    elif score['neg'] > 0.15 and score['neg'] > score['pos']:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Baseline_Neutral'\n",
    "\n",
    "#There are neutral scores = 1.0, so need to keep that in mind.\n",
    "#Since median 0.791, choose threshold above median. Examples: 0.8, 0.85, 0.9, 0.95, 0.99\n",
    "polarity = [0]*len(upworthy)\n",
    "for i in range(len(headline)):\n",
    "    polarity[i] = vader_polarity_pnn(headline[i])\n",
    "\n",
    "#add polarity to upworthy dataframe\n",
    "upworthy['polarity_vader'] = polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                    CTR   R-squared:                       0.002\n",
      "Model:                            OLS   Adj. R-squared:                  0.002\n",
      "Method:                 Least Squares   F-statistic:                     22.28\n",
      "Date:                Wed, 20 Jan 2021   Prob (F-statistic):           2.16e-10\n",
      "Time:                        15:15:19   Log-Likelihood:                 67570.\n",
      "No. Observations:               22666   AIC:                        -1.351e+05\n",
      "Df Residuals:                   22663   BIC:                        -1.351e+05\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=================================================================================================\n",
      "                                    coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------------------\n",
      "Intercept                         0.0159      0.000    118.835      0.000       0.016       0.016\n",
      "C(polarity_vader)[T.Negative]     0.0006      0.000      2.738      0.006       0.000       0.001\n",
      "C(polarity_vader)[T.Positive]    -0.0008      0.000     -4.173      0.000      -0.001      -0.000\n",
      "==============================================================================\n",
      "Omnibus:                    10652.535   Durbin-Watson:                   0.586\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            74141.571\n",
      "Skew:                           2.156   Prob(JB):                         0.00\n",
      "Kurtosis:                      10.740   Cond. No.                         3.57\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "#OLS regression pos-neg-neu\n",
    "df_polarity = upworthy[['CTR','polarity_vader']]\n",
    "polarity_model = smf.ols(formula = 'CTR ~ C(polarity_vader)', data = df_polarity).fit()\n",
    "print(polarity_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "upworthy.to_csv(\"upworthy_vader_pnn_classifier.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classifier with defined thresholds, using compound\n",
    "def vader_polarity_compound(text):\n",
    "    score = vader.polarity_scores(text)\n",
    "    if score['compound'] <= 0.5 and score['compound'] >= -0.5:\n",
    "        return 'Baseline_Neutral'\n",
    "    elif score['compound'] > 0.5:\n",
    "        return 'Positive'\n",
    "    elif score['compound'] < -0.5:\n",
    "        return 'Negative'\n",
    "\n",
    "#There are neutral scores = 1.0, so need to keep that in mind.\n",
    "#Since median 0.791, choose threshold above median. Examples: 0.8, 0.85, 0.9, 0.95, 0.99\n",
    "polarity = [0]*len(upworthy)\n",
    "for i in range(len(headline.index)):\n",
    "    polarity[i] = vader_polarity_compound(headline[i])\n",
    "\n",
    "#add polarity to upworthy dataframe\n",
    "upworthy['polarity_vader'] = polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                    CTR   R-squared:                       0.002\n",
      "Model:                            OLS   Adj. R-squared:                  0.002\n",
      "Method:                 Least Squares   F-statistic:                     26.39\n",
      "Date:                Wed, 20 Jan 2021   Prob (F-statistic):           3.55e-12\n",
      "Time:                        15:15:27   Log-Likelihood:                 67575.\n",
      "No. Observations:               22666   AIC:                        -1.351e+05\n",
      "Df Residuals:                   22663   BIC:                        -1.351e+05\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=================================================================================================\n",
      "                                    coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------------------\n",
      "Intercept                         0.0159   9.89e-05    160.603      0.000       0.016       0.016\n",
      "C(polarity_vader)[T.Negative]     0.0007      0.000      2.873      0.004       0.000       0.001\n",
      "C(polarity_vader)[T.Positive]    -0.0013      0.000     -6.005      0.000      -0.002      -0.001\n",
      "==============================================================================\n",
      "Omnibus:                    10663.201   Durbin-Watson:                   0.587\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            74362.286\n",
      "Skew:                           2.158   Prob(JB):                         0.00\n",
      "Kurtosis:                      10.753   Cond. No.                         3.22\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "#OLS regression compound\n",
    "df_polarity = upworthy[['CTR','polarity_vader']]\n",
    "polarity_model = smf.ols(formula = 'CTR ~ C(polarity_vader)', data = df_polarity).fit()\n",
    "print(polarity_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "upworthy.to_csv(\"upworthy_vader_compound_classifier.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextBlob Model\n",
    "TextBlob is another sentiment analyzer; however, it also scores subjectivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "#grab a fresh copy of upworthy\n",
    "upworthy = pd.read_csv(upworthy_data_dir, index_col = 1)\n",
    "CTR = np.array(upworthy['clicks'])/np.array(upworthy['impressions'])\n",
    "upworthy['CTR'] = CTR\n",
    "headline = upworthy.headline\n",
    "\n",
    "headline_blob = [0]*len(headline)\n",
    "\n",
    "for i in range(len(headline)):\n",
    "    headline_blob[i] = TextBlob(headline[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_polarity = [0]*len(headline_blob)\n",
    "blob_subjectivity = [0]*len(headline_blob)\n",
    "\n",
    "for i in range(len(headline_blob)):\n",
    "    blob_polarity[i] = headline_blob[i].sentiment[0]\n",
    "    blob_subjectivity[i] = headline_blob[i].sentiment[1]\n",
    "    \n",
    "upworthy['polarity'] = blob_polarity\n",
    "upworthy['subjectivity'] = blob_subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                    CTR   R-squared:                       0.001\n",
      "Model:                            OLS   Adj. R-squared:                  0.001\n",
      "Method:                 Least Squares   F-statistic:                     8.076\n",
      "Date:                Wed, 20 Jan 2021   Prob (F-statistic):           0.000312\n",
      "Time:                        15:15:40   Log-Likelihood:                 67556.\n",
      "No. Observations:               22666   AIC:                        -1.351e+05\n",
      "Df Residuals:                   22663   BIC:                        -1.351e+05\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "================================================================================\n",
      "                   coef    std err          t      P>|t|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------\n",
      "Intercept        0.0159      0.000    117.543      0.000       0.016       0.016\n",
      "polarity        -0.0010      0.000     -3.920      0.000      -0.002      -0.001\n",
      "subjectivity    -0.0001      0.000     -0.405      0.685      -0.001       0.000\n",
      "==============================================================================\n",
      "Omnibus:                    10667.320   Durbin-Watson:                   0.586\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            74566.488\n",
      "Skew:                           2.158   Prob(JB):                         0.00\n",
      "Kurtosis:                      10.767   Cond. No.                         3.77\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "#OLS regression\n",
    "df_sentiment = upworthy[['CTR','polarity','subjectivity']]\n",
    "sentiment_model = smf.ols(formula = 'CTR ~ polarity+ subjectivity', data = df_sentiment).fit()\n",
    "print(sentiment_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "upworthy.to_csv(\"upworthy_textblob_classifier.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
